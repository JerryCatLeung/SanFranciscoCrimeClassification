{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SF crime classification\n",
    "The first part is done by Borui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.406939592\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "\n",
    "from sklearn import preprocessing, cross_validation, svm\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "\"\"\"\n",
    "Data fields\n",
    "\n",
    "Dates - timestamp of the crime incident\n",
    "Category - category of the crime incident (only in train.csv). This is the target variable you are going to predict.\n",
    "Descript - detailed description of the crime incident (only in train.csv)\n",
    "DayOfWeek - the day of the week\n",
    "PdDistrict - name of the Police Department District\n",
    "Resolution - how the crime incident was resolved (only in train.csv)\n",
    "Address - the approximate street address of the crime incident\n",
    "X - Longitude\n",
    "Y - Latitude\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#z1 = zipfile.ZipFile('train.zip')\n",
    "#z2 = zipfile.ZipFile('test.zip')\n",
    "train = pd.read_csv('train.csv', parse_dates = ['Dates'])\n",
    "test = pd.read_csv('test.csv', parse_dates = ['Dates'])\n",
    "\n",
    "\n",
    "#Convert crime category (labels) to numbers\n",
    "le_crime = preprocessing.LabelEncoder()\n",
    "crime = le_crime.fit_transform(train.Category)\n",
    "\n",
    "days = pd.get_dummies(train.DayOfWeek)\n",
    "district = pd.get_dummies(train.PdDistrict)\n",
    "year = train.Dates.dt.year\n",
    "hour = train.Dates.dt.hour\n",
    "day = train.Dates.dt.day\n",
    "x = train.X\n",
    "y = train.Y\n",
    "\n",
    "#Build new array and create train data and train label\n",
    "train_data = pd.concat([year, day, hour, days, district, x, y], axis=1)\n",
    "train_data['crime'] = crime\n",
    "\n",
    "crime_data = train_data.iloc[:,:-1]\n",
    "crime_label = train_data['crime']\n",
    "\n",
    "days = pd.get_dummies(test.DayOfWeek)\n",
    "district = pd.get_dummies(test.PdDistrict)\n",
    "year = test.Dates.dt.year\n",
    "hour = test.Dates.dt.hour\n",
    "day = test.Dates.dt.day\n",
    "x = test.X\n",
    "y = test.Y\n",
    "test_data = pd.concat([year, day, hour, days, district, x, y], axis=1)\n",
    "\n",
    "[crime_train_data, crime_test_data, crime_train_labels, crime_test_labels] = cross_validation.train_test_split(crime_data, crime_label, test_size=0.3)\n",
    "\n",
    "#Logistic Regression\n",
    "#lgr = LogisticRegression(C=1e5)\n",
    "#lgr.fit(crime_train_data, crime_train_labels.values.ravel())\n",
    "#prediction = lgr.predict_proba(crime_test_data)\n",
    "#log_loss(crime_test_labels, prediction)\n",
    "#print lgr.score(crime_test_data, crime_test_labels)\n",
    "#print accuracy_score(crime_test_data, crime_test_labels)\n",
    "# 2.58\n",
    "\n",
    "# Naive Bayes\n",
    "#crime_nb = BernoulliNB()\n",
    "#crime_nb.fit(crime_train_data, crime_train_labels)\n",
    "#prediction = np.array(crime_nb.predict_proba(crime_test_data))\n",
    "#log_loss(crime_test_labels, prediction) \n",
    "#print accuracy_score(crime_test_labels, prediction)\n",
    "\n",
    "# SVM\n",
    "#crime_svm = svm.SVC(kernel='linear')\n",
    "#learn the SVM\n",
    "#crime_svm.fit(crime_train_data, crime_train_labels)\n",
    "#predicted = np.array(crime_svm.predict_proba(crime_test_data))\n",
    "#log_loss(crime_test_labels, predicted) \n",
    "#print crime_svm.score(crime_test_data, crime_test_labels)\n",
    "\n",
    "# Random Forest\n",
    "crime_rf = RandomForestClassifier()\n",
    "crime_rf.fit(crime_train_data, crime_train_labels)\n",
    "prediction = np.array(crime_rf.predict_proba(crime_test_data))\n",
    "#print accuracy_score(crime_test_labels, crime_rf.predict_proba(crime_test_data))\n",
    "print log_loss(crime_test_labels, prediction) \n",
    "\n",
    "#result=pd.DataFrame(prediction, columns=le_crime.classes_)\n",
    "#result.to_csv('testResult.csv', index = True, index_label = 'Id' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Improvement\n",
    "This part is done by Han Chen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Mar 10 13:16:51 2016\n",
    "\n",
    "@author: Kevin\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn import preprocessing, cross_validation\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sknn.mlp import Classifier, Layer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "clear = lambda: os.system('cls')\n",
    "clear()\n",
    "\n",
    "train = pd.read_csv('../train.csv', parse_dates = ['Dates'])\n",
    "\n",
    "#train = train.head(n=10)\n",
    "train['AddressContainOf'] = 0\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "lenth = len(train)\n",
    "cur = 0\n",
    "for index in range(len(train)):\n",
    "    if(index > 5000 and index/5000.0 > cur):\n",
    "        print \"pre-processing the\", cur*5000, \"th data\"\n",
    "        cur += 1\n",
    "    if(train.iloc[index,6].find('/') == -1):\n",
    "        train.iloc[index,9] = 1\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "coor = []\n",
    "for index in range(len(train)):\n",
    "    coor.append([train['X'].iloc[index], train['Y'].iloc[index]])\n",
    "pca.fit(coor)\n",
    "pca_transform = pca.transform(coor)\n",
    "cur = 0\n",
    "for index in range(len(train)):\n",
    "    if(index > 5000 and index/5000.0 > cur):\n",
    "        print \"pre-processing the\", cur*5000, \"th data\"\n",
    "        cur += 1\n",
    "    train.iloc[index,7] = pca_transform[index][0]\n",
    "    train.iloc[index,8] = pca_transform[index][1]\n",
    "    \n",
    "train.to_csv('newtrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Mar 10 13:16:51 2016\n",
    "\n",
    "@author: Kevin\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn import preprocessing, cross_validation\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sknn.mlp import Classifier, Layer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "clear = lambda: os.system('cls')\n",
    "clear()\n",
    "\n",
    "train = pd.read_csv('newtrain.csv', parse_dates = ['Dates'])\n",
    "train = train.head(n=100000)\n",
    "\n",
    "print train.head()\n",
    "\n",
    "\n",
    "train['StreetNo'] = train['Address'].apply(lambda x: x.split(' ', 1)[0] if x.split(' ', 1)[0].isdigit() else 0)\n",
    "#train['Address'] = train['Address'].apply(lambda x: x.split(' ', 1)[1] if x.split(' ', 1)[0].isdigit() else x)\n",
    "train['hour'] = train['Dates'].dt.hour\n",
    "train['evening'] = train['Dates'].dt.hour.isin([18,19,20,21,22,23,0,1,2,3,4,5,6])\n",
    "train['Year'] = train['Dates'].dt.year\n",
    "#train = train[train['Year'].isin([2011,2012,2013,2014,2015])]\n",
    "train['Month'] = train['Dates'].dt.month\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "lenth = len(train)\n",
    "cur = 0\n",
    "\n",
    "print '  -> processing time:', time.time() - start\n",
    "#print train.head()\n",
    "print len(set(train['StreetNo'])), len(set(train['Address']))\n",
    "\n",
    "le = LabelEncoder()\n",
    "crime = le.fit_transform(train.Category)\n",
    "\n",
    "hour = pd.get_dummies(train.hour)\n",
    "district = pd.get_dummies(train.PdDistrict)\n",
    "StreetNo = pd.get_dummies(train.StreetNo)\n",
    "evening = pd.get_dummies(train.evening)\n",
    "ContainOf = pd.get_dummies(train.AddressContainOf)\n",
    "Year = pd.get_dummies(train.Year)\n",
    "Month = pd.get_dummies(train.Month)\n",
    "\n",
    "train_data = pd.concat([hour, district, StreetNo, evening, ContainOf, train['X'], train['Y']], axis=1)\n",
    "train_data['crime'] = crime\n",
    "crime_data = train_data.iloc[:,:-1]\n",
    "crime_label = train_data['crime']\n",
    "\n",
    "classifiers = [\n",
    "    BernoulliNB(),\n",
    "    RandomForestClassifier(max_depth=10, n_estimators=1024, n_jobs=-1),\n",
    "    RandomForestClassifier(max_depth=12, n_estimators=1024, n_jobs=-1),\n",
    "    RandomForestClassifier(max_depth=14, n_estimators=1024, n_jobs=-1),\n",
    "    RandomForestClassifier(max_depth=16, n_estimators=1024, n_jobs=-1),\n",
    "    RandomForestClassifier(max_depth=18, n_estimators=1024, n_jobs=-1),\n",
    "    RandomForestClassifier(max_depth=20, n_estimators=1024, n_jobs=-1),\n",
    "    RandomForestClassifier(max_depth=22, n_estimators=1024, n_jobs=-1),\n",
    "    KNeighborsClassifier(n_neighbors=100, weights='distance', algorithm='ball_tree', leaf_size=100, p=10, metric='minkowski'),\n",
    "    #XGBClassifier(max_depth=16,n_estimators=1024),\n",
    "    GradientBoostingClassifier(n_estimators=10, learning_rate=1.0,max_depth=5, random_state=0),\n",
    "    AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=8), algorithm=\"SAMME.R\", n_estimators=128),\n",
    "    ]\n",
    "    \n",
    "#print train.head()\n",
    "    \n",
    "newClassifiers = [\n",
    "    BernoulliNB(),\n",
    "    RandomForestClassifier(max_depth=16, n_estimators=1024, n_jobs=-1),\n",
    "    GradientBoostingClassifier(max_depth=16, n_estimators=1024)\n",
    "    #KNeighborsClassifier(n_neighbors=50, weights='distance', algorithm='ball_tree', leaf_size=100, p=10, metric='minkowski', n_jobs=-1),\n",
    "    ]\n",
    "\n",
    " \n",
    "#[train_d, test_d, train_labels, test_labels] = cross_validation.train_test_split(crime_data, crime_label, test_size=0.2, random_state=20160217)\n",
    "skf = cross_validation.StratifiedKFold(crime_label, n_folds=2, random_state=20160217, shuffle=True)\n",
    "for train_index, test_index in skf:\n",
    "    train_d, test_d = crime_data.iloc[train_index,:], crime_data.iloc[test_index,:]\n",
    "    train_labels, test_labels = crime_label[train_index], crime_label[test_index]\n",
    "    print train_d.shape, test_d.shape\n",
    "    for classifier in classifiers:\n",
    "        print classifier.__class__.__name__\n",
    "        start = time.time()\n",
    "        classifier.fit(train_d, train_labels)\n",
    "        print '  -> Training time:', time.time() - start\n",
    "                        \n",
    "        start = time.time()        \n",
    "        #score_result = classifier.score(test_d, test_labels)\n",
    "        #print '  -> caluclate score time', time.time() - start\n",
    "                        \n",
    "        start = time.time()\n",
    "        predicted = np.array(classifier.predict_proba(test_d))\n",
    "        print '  -> predict_proba time:', time.time() - start\n",
    "                        \n",
    "        start = time.time()\n",
    "        log_result = log_loss(test_labels, predicted)\n",
    "        print '  -> calculate log_loss time:', time.time() - start        \n",
    "                        \n",
    "        #print \"score = \", score_result, \"log loss = \",log_result\n",
    "        print \"log_loss = \", log_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
